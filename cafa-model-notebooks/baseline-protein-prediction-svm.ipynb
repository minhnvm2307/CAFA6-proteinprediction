{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T15:02:02.286534Z",
     "iopub.status.busy": "2025-12-13T15:02:02.286170Z",
     "iopub.status.idle": "2025-12-13T15:02:02.293856Z",
     "shell.execute_reply": "2025-12-13T15:02:02.292651Z",
     "shell.execute_reply.started": "2025-12-13T15:02:02.286501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Verify data directory\n",
    "DATA_DIR = '../data'\n",
    "if os.path.exists(DATA_DIR):\n",
    "    print(f\"✓ Data directory found: {DATA_DIR}\")\n",
    "else:\n",
    "    print(f\"✗ Data directory not found: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Baseline Model for Protein Function Prediction\n",
    "\n",
    "This notebook trains a Linear SVM model using CTD features (Composition, Transition, Distribution) for protein function prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T15:02:04.396056Z",
     "iopub.status.busy": "2025-12-13T15:02:04.395662Z",
     "iopub.status.idle": "2025-12-13T15:02:04.404358Z",
     "shell.execute_reply": "2025-12-13T15:02:04.403127Z",
     "shell.execute_reply.started": "2025-12-13T15:02:04.396031Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calculating Naive Frequency given a sequence\n",
    "def naive_freq(sequence):\n",
    "    \"\"\"\n",
    "    Input: an amino acid sequence of variable length\n",
    "    Output: a list of shape (1, 20), each element standing for an amino acid and its frequency\n",
    "    \"\"\"\n",
    "    # Counting the appearance of each amino acid within the sequence\n",
    "    AMINO_ACIDS = 'ARNDCQEGHILKMFPSTWYV'\n",
    "\n",
    "    AA_TO_INDEX = {aa: i for i, aa in enumerate(AMINO_ACIDS)}\n",
    "\n",
    "    frequency_vector = [0] * 20\n",
    "\n",
    "    for amino_acid in sequence:\n",
    "        try:\n",
    "            index = AA_TO_INDEX[amino_acid]\n",
    "            frequency_vector[index] += 1\n",
    "        except KeyError:\n",
    "            print(f\"Warning: Skipping unknown amino acid {amino_acid}\")\n",
    "            pass\n",
    "\n",
    "    # Calculating frequencies for each amino acid\n",
    "    total_length = len(sequence)\n",
    "\n",
    "    normalized_vector = frequency_vector\n",
    "    if total_length > 0:\n",
    "        normalized_vector = [count / total_length for count in frequency_vector]\n",
    "\n",
    "    return normalized_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T15:02:06.470885Z",
     "iopub.status.busy": "2025-12-13T15:02:06.469853Z",
     "iopub.status.idle": "2025-12-13T15:02:06.481293Z",
     "shell.execute_reply": "2025-12-13T15:02:06.479934Z",
     "shell.execute_reply.started": "2025-12-13T15:02:06.470853Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CTD_GROUPS_BY_LIST = {\n",
    "    \"Hydrophobicity\": {\n",
    "        1: {\"A\", \"V\", \"L\", \"I\", \"M\", \"F\", \"W\", \"C\"},               # Hydrophobic\n",
    "        2: {\"G\", \"H\", \"Y\", \"P\", \"T\", \"S\"},          # Neutral\n",
    "        3: {\"R\", \"K\", \"Q\", \"E\", \"D\", \"N\"}           # Polar\n",
    "    },\n",
    "\n",
    "    \"Charge\": {\n",
    "        1: {\"D\", \"E\"},      # Negative\n",
    "        2: {\"A\", \"G\", \"I\", \"L\", \"M\", \"F\", \"P\", \"Q\", \"S\", \"T\", \"W\", \"Y\", \"V\", \"N\", \"C\"}, # Neutral\n",
    "        3: {\"K\", \"R\", \"H\"}     # Positive\n",
    "    },\n",
    "\n",
    "    \"VanDerWaals\": {\n",
    "        1: {\"A\", \"G\", \"S\", \"C\"},  # Small\n",
    "        2: {\"T\", \"D\", \"P\", \"N\", \"V\"},   # Medium\n",
    "        3: {\"E\", \"Q\", \"L\", \"I\", \"F\", \"Y\", \"M\", \"H\", \"K\", \"R\", \"W\"}   # Large\n",
    "    },\n",
    "\n",
    "    \"Polarity\": {\n",
    "        1: {\"L\", \"A\", \"W\", \"F\", \"C\", \"M\", \"V\", \"I\", \"Y\"},   # Small\n",
    "        2: {\"P\", \"T\", \"S\", \"G\", \"H\"},    # Medium\n",
    "        3: {\"Q\", \"N\", \"E\", \"D\", \"K\", \"R\"}   # High\n",
    "    },\n",
    "\n",
    "    \"Polarizability\": {\n",
    "        1: {\"G\", \"A\", \"S\", \"D\", \"C\"},   # Small\n",
    "        2: {\"T\", \"P\", \"N\", \"H\", \"E\", \"Q\", \"K\"},   # Medium\n",
    "        3: {\"M\", \"I\", \"L\", \"V\", \"F\", \"Y\", \"W\", \"R\"}\n",
    "    },\n",
    "\n",
    "    \"SecondStructure\": {\n",
    "        1: {\"E\", \"A\", \"L\", \"M\", \"Q\", \"K\", \"R\", \"H\"},  # Helix\n",
    "        2: {\"V\", \"I\", \"Y\", \"C\", \"W\", \"F\", \"T\"},  # Strand\n",
    "        3: {\"G\", \"N\", \"P\", \"S\", \"D\"}  # Coil\n",
    "    },\n",
    "\n",
    "    \"Solvent\": {\n",
    "        1: {\"A\", \"L\", \"F\", \"C\", \"G\", \"I\", \"V\", \"W\"},  # Buried\n",
    "        2: {\"R\", \"K\", \"Q\", \"E\", \"D\", \"N\"},  # Intermediate\n",
    "        3: {\"M\", \"S\", \"P\", \"T\", \"H\", \"Y\"}  # Exposed\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_group(aa, property_map):\n",
    "    for g, aa_set in property_map.items():\n",
    "        if aa in aa_set:\n",
    "            return g\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T15:02:08.821732Z",
     "iopub.status.busy": "2025-12-13T15:02:08.821385Z",
     "iopub.status.idle": "2025-12-13T15:02:08.836129Z",
     "shell.execute_reply": "2025-12-13T15:02:08.834857Z",
     "shell.execute_reply.started": "2025-12-13T15:02:08.821708Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def aa_ctd(sequence, physicochem):\n",
    "    \"\"\"\n",
    "    For this composition, we compute the statistics on groups divided by physicochemical properties:\n",
    "        e.g: By hydrophobicity, we have three groups:\n",
    "            Class 1: {A, G, V, L, I, M, F, W, P}\n",
    "            Class 2: {R, K, H}\n",
    "            Class 3: {S, T, Y, C, N, Q, D, E}\n",
    "\n",
    "    Amino Acid composition is made up of three subcompositions for each physicochemical property:\n",
    "    - Composition (C): The frequency of a group by the number of AAs\n",
    "    - Transition (T) :\n",
    "    Input: an amino acid sequence of variable length\n",
    "\n",
    "    \"\"\"\n",
    "    property_map = CTD_GROUPS_BY_LIST[physicochem]\n",
    "    L = len(sequence)\n",
    "\n",
    "    groups = []\n",
    "    for aa in sequence:\n",
    "        g = get_group(aa, property_map)\n",
    "        if g is not None:\n",
    "            groups.append(g)\n",
    "\n",
    "    L = len(groups)\n",
    "    if L == 0:\n",
    "        return [0.0] * 21  # safe fallback\n",
    "\n",
    "    # Count members of each group\n",
    "    N = {1: 0, 2: 0, 3: 0}\n",
    "    for g in groups:\n",
    "        N[g] += 1\n",
    "\n",
    "    composition = [N[1]/L, N[2]/L, N[3]/L]\n",
    "\n",
    "    # Transitions\n",
    "    T12 = T13 = T23 = 0\n",
    "    for i in range(L - 1):\n",
    "        g1, g2 = groups[i], groups[i + 1]\n",
    "        if g1 == g2:\n",
    "            continue\n",
    "        gmin, gmax = min(g1, g2), max(g1, g2)\n",
    "        if gmin == 1 and gmax == 2:\n",
    "            T12 += 1\n",
    "        elif gmin == 1 and gmax == 3:\n",
    "            T13 += 1\n",
    "        elif gmin == 2 and gmax == 3:\n",
    "            T23 += 1\n",
    "\n",
    "    denom = L - 1\n",
    "    transition = [T12/denom, T13/denom, T23/denom]\n",
    "\n",
    "    # Distribution\n",
    "    positions = {1: [], 2: [], 3: []}\n",
    "    for i, g in enumerate(groups):\n",
    "        positions[g].append(i + 1)\n",
    "\n",
    "    P_k = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "    distribution = []\n",
    "\n",
    "    for g in [1, 2, 3]:\n",
    "        pos_list = positions[g]\n",
    "        Ng = len(pos_list)\n",
    "        if Ng == 0:\n",
    "            distribution.extend([0.0]*5)\n",
    "            continue\n",
    "        for pk in P_k:\n",
    "            if pk == 0:\n",
    "                idx = 0\n",
    "            else:\n",
    "                idx = math.ceil(Ng * pk) - 1\n",
    "            distribution.append(pos_list[idx] / L)\n",
    "\n",
    "    return composition + transition + distribution\n",
    "\n",
    "sequence = \"AEAAAEAEEAAAAAEAEEEAAEEAEEEAAE\"\n",
    "ctd = aa_ctd(sequence, \"Hydrophobicity\")\n",
    "print(len(ctd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T15:02:11.447209Z",
     "iopub.status.busy": "2025-12-13T15:02:11.446878Z",
     "iopub.status.idle": "2025-12-13T15:02:11.453619Z",
     "shell.execute_reply": "2025-12-13T15:02:11.452514Z",
     "shell.execute_reply.started": "2025-12-13T15:02:11.447186Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def dipeptide_composition(seq):\n",
    "    \"\"\"Compute dipeptide composition.\"\"\"\n",
    "    AA = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    dipeptides = [a+b for a in AA for b in AA]\n",
    "    seq = seq.upper()\n",
    "    length = len(seq)-1 if len(seq)>1 else 1\n",
    "    return [sum(1 for i in range(len(seq)-1) if seq[i]+seq[i+1]==dp)/length for dp in dipeptides]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T15:02:13.532146Z",
     "iopub.status.busy": "2025-12-13T15:02:13.531807Z",
     "iopub.status.idle": "2025-12-13T15:02:13.537789Z",
     "shell.execute_reply": "2025-12-13T15:02:13.536642Z",
     "shell.execute_reply.started": "2025-12-13T15:02:13.532122Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_accession(header):\n",
    "    \"\"\"Extract accession from a FASTA header like >sp|A0JP26|POTB3_HUMAN\"\"\"\n",
    "    parts = header.lstrip('>').split('|')\n",
    "    if len(parts) >= 2:\n",
    "        return parts[1]\n",
    "    else:\n",
    "        return header.lstrip('>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T15:02:15.475204Z",
     "iopub.status.busy": "2025-12-13T15:02:15.474883Z",
     "iopub.status.idle": "2025-12-13T15:02:15.573828Z",
     "shell.execute_reply": "2025-12-13T15:02:15.572604Z",
     "shell.execute_reply.started": "2025-12-13T15:02:15.475182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Feature extraction from FASTA files\n",
    "bio_properties = [\"Hydrophobicity\", \"Charge\", \"VanDerWaals\", \"Polarity\",\n",
    "                  \"Polarizability\", \"SecondStructure\", \"Solvent\"]\n",
    "\n",
    "def load_fasta_features(fasta_path):\n",
    "    \"\"\"\n",
    "    Load sequences from a FASTA file, extract features, and return\n",
    "    a feature matrix X and a list of protein IDs.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(fasta_path):\n",
    "        print(f\"Error: {fasta_path} not found!\")\n",
    "        return [], np.array([])\n",
    "    \n",
    "    ids = []\n",
    "    feats = []\n",
    "\n",
    "    for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "        protein_id = extract_accession(record.id)\n",
    "        seq = str(record.seq).upper()\n",
    "\n",
    "        # Feature extraction\n",
    "        x = []\n",
    "        x.extend(naive_freq(seq))\n",
    "        for prop in bio_properties:\n",
    "            x.extend(aa_ctd(seq, prop))\n",
    "        x.extend(dipeptide_composition(seq))\n",
    "\n",
    "        ids.append(protein_id)\n",
    "        feats.append(x)\n",
    "\n",
    "    X = np.vstack(feats)\n",
    "    print(f\"Loaded {len(ids)} sequences with {X.shape[1]} features each\")\n",
    "    return ids, X\n",
    "\n",
    "# Load training data\n",
    "train_fasta_path = \"../data/Train/train_sequences.fasta\"\n",
    "train_ids, X_train = load_fasta_features(train_fasta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T15:02:17.244480Z",
     "iopub.status.busy": "2025-12-13T15:02:17.243446Z",
     "iopub.status.idle": "2025-12-13T15:02:18.473324Z",
     "shell.execute_reply": "2025-12-13T15:02:18.472144Z",
     "shell.execute_reply.started": "2025-12-13T15:02:17.244447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load training labels from train_terms.tsv\n",
    "train_terms_path = \"../data/Train/train_terms.tsv\"\n",
    "\n",
    "if os.path.exists(train_terms_path):\n",
    "    df = pd.read_csv(train_terms_path, sep=\"\\t\", names=[\"EntryID\", \"GO\", \"Ont\"])\n",
    "    \n",
    "    df[\"EntryID\"] = df[\"EntryID\"].str.strip()\n",
    "    df[\"GO\"] = df[\"GO\"].str.strip()\n",
    "    df[\"Ont\"] = df[\"Ont\"].str.strip()\n",
    "    \n",
    "    # Split labels by ontology\n",
    "    labels_MF = {}  # Molecular Function\n",
    "    labels_BP = {}  # Biological Process\n",
    "    labels_CC = {}  # Cellular Component\n",
    "    \n",
    "    for entry, go, ont in zip(df[\"EntryID\"], df[\"GO\"], df[\"Ont\"]):\n",
    "        if ont == \"F\":\n",
    "            labels_MF.setdefault(entry, []).append(go)\n",
    "        elif ont == \"P\":\n",
    "            labels_BP.setdefault(entry, []).append(go)\n",
    "        elif ont == \"C\":\n",
    "            labels_CC.setdefault(entry, []).append(go)\n",
    "    \n",
    "    print(f\"Loaded labels for:\")\n",
    "    print(f\"  - Molecular Function (MF): {len(labels_MF)} proteins\")\n",
    "    print(f\"  - Biological Process (BP): {len(labels_BP)} proteins\")\n",
    "    print(f\"  - Cellular Component (CC): {len(labels_CC)} proteins\")\n",
    "else:\n",
    "    print(f\"Error: {train_terms_path} not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T15:02:19.439843Z",
     "iopub.status.busy": "2025-12-13T15:02:19.439472Z",
     "iopub.status.idle": "2025-12-13T15:02:19.448667Z",
     "shell.execute_reply": "2025-12-13T15:02:19.446639Z",
     "shell.execute_reply.started": "2025-12-13T15:02:19.439814Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Build label matrices for each ontology\n",
    "def build_Y(train_ids, label_dict):\n",
    "    \"\"\"Build binary label matrix from label dictionary\"\"\"\n",
    "    # Collect all GO terms for this ontology\n",
    "    all_terms = sorted({go for gos in label_dict.values() for go in gos})\n",
    "    term_to_index = {go: i for i, go in enumerate(all_terms)}\n",
    "\n",
    "    Y = np.zeros((len(train_ids), len(all_terms)), dtype=np.uint8)\n",
    "\n",
    "    for i, pid in enumerate(train_ids):\n",
    "        if pid in label_dict:\n",
    "            for go in label_dict[pid]:\n",
    "                j = term_to_index[go]\n",
    "                Y[i, j] = 1\n",
    "\n",
    "    return Y, term_to_index, all_terms\n",
    "\n",
    "# Build label matrices\n",
    "Y_MF_UNFILTERED, mf_term_to_idx, mf_terms = build_Y(train_ids, labels_MF)\n",
    "Y_BP_UNFILTERED, bp_term_to_idx, bp_terms = build_Y(train_ids, labels_BP)\n",
    "Y_CC_UNFILTERED, cc_term_to_idx, cc_terms = build_Y(train_ids, labels_CC)\n",
    "\n",
    "print(f\"Label matrix shapes (before filtering):\")\n",
    "print(f\"  MF: {Y_MF_UNFILTERED.shape}\")\n",
    "print(f\"  BP: {Y_BP_UNFILTERED.shape}\")\n",
    "print(f\"  CC: {Y_CC_UNFILTERED.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T10:35:25.031749Z",
     "iopub.status.busy": "2025-12-13T10:35:25.031251Z",
     "iopub.status.idle": "2025-12-13T10:47:52.836122Z",
     "shell.execute_reply": "2025-12-13T10:47:52.834639Z",
     "shell.execute_reply.started": "2025-12-13T10:35:25.031715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train SVM models for each ontology\n",
    "print(\"Training SVM models...\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_reduced)\n",
    "\n",
    "# SGDClassifier (linear SVM equivalent)\n",
    "base = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3, class_weight='balanced', random_state=42)\n",
    "\n",
    "# Train models for each ontology\n",
    "print(\"Training MF model...\")\n",
    "svm_mf = MultiOutputClassifier(base, n_jobs=-1)\n",
    "svm_mf.fit(X_train_scaled, Y_MF)\n",
    "\n",
    "print(\"Training BP model...\")\n",
    "svm_bp = MultiOutputClassifier(base, n_jobs=-1)\n",
    "svm_bp.fit(X_train_scaled, Y_BP)\n",
    "\n",
    "print(\"Training CC model...\")\n",
    "svm_cc = MultiOutputClassifier(base, n_jobs=-1)\n",
    "svm_cc.fit(X_train_scaled, Y_CC)\n",
    "\n",
    "print(\"All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T09:21:43.574998Z",
     "iopub.status.busy": "2025-12-13T09:21:43.573787Z",
     "iopub.status.idle": "2025-12-13T09:21:46.846340Z",
     "shell.execute_reply": "2025-12-13T09:21:46.845305Z",
     "shell.execute_reply.started": "2025-12-13T09:21:43.574959Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Filter rare labels (keep only labels with >= min_pos annotations)\n",
    "min_pos = 50\n",
    "\n",
    "keep_MF = (Y_MF_UNFILTERED.sum(axis=0) >= min_pos)\n",
    "keep_BP = (Y_BP_UNFILTERED.sum(axis=0) >= min_pos)\n",
    "keep_CC = (Y_CC_UNFILTERED.sum(axis=0) >= min_pos)\n",
    "\n",
    "Y_MF = Y_MF_UNFILTERED[:, keep_MF]\n",
    "Y_BP = Y_BP_UNFILTERED[:, keep_BP]\n",
    "Y_CC = Y_CC_UNFILTERED[:, keep_CC]\n",
    "\n",
    "# Filter term lists to match filtered label matrices\n",
    "mf_terms_filtered = np.array(mf_terms)[keep_MF]\n",
    "bp_terms_filtered = np.array(bp_terms)[keep_BP]\n",
    "cc_terms_filtered = np.array(cc_terms)[keep_CC]\n",
    "\n",
    "print(f\"After filtering (min_pos={min_pos}):\")\n",
    "print(f\"  Remaining labels MF: {Y_MF.shape[1]}\")\n",
    "print(f\"  Remaining labels BP: {Y_BP.shape[1]}\")\n",
    "print(f\"  Remaining labels CC: {Y_CC.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T09:01:08.200224Z",
     "iopub.status.busy": "2025-12-13T09:01:08.199913Z",
     "iopub.status.idle": "2025-12-13T09:01:15.917382Z",
     "shell.execute_reply": "2025-12-13T09:01:15.916121Z",
     "shell.execute_reply.started": "2025-12-13T09:01:08.200202Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dimensionality reduction using PCA\n",
    "print(\"Applying PCA for dimensionality reduction...\")\n",
    "\n",
    "pca = PCA(n_components=0.95)  # Keep 95% variance\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "print(f\"Original dimensions: {X_train.shape[1]}\")\n",
    "print(f\"Reduced dimensions (95% variance): {X_train_reduced.shape[1]}\")\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T08:58:10.496284Z",
     "iopub.status.busy": "2025-12-13T08:58:10.495917Z",
     "iopub.status.idle": "2025-12-13T08:58:18.180252Z",
     "shell.execute_reply": "2025-12-13T08:58:18.178773Z",
     "shell.execute_reply.started": "2025-12-13T08:58:10.496260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_fasta_path = \"../data/Test/testsuperset.fasta\"\n",
    "\n",
    "if os.path.exists(test_fasta_path):\n",
    "    test_ids, X_test = load_fasta_features(test_fasta_path)\n",
    "    \n",
    "    # Apply same transformations as training data\n",
    "    X_test_reduced = pca.transform(X_test)\n",
    "    X_test_scaled = scaler.transform(X_test_reduced)\n",
    "    \n",
    "    print(f\"Test data loaded: {X_test.shape[0]} samples\")\n",
    "    print(f\"Test data reduced shape: {X_test_reduced.shape}\")\n",
    "else:\n",
    "    print(f\"Warning: {test_fasta_path} not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T11:25:59.118944Z",
     "iopub.status.busy": "2025-12-13T11:25:59.118603Z",
     "iopub.status.idle": "2025-12-13T11:25:59.891452Z",
     "shell.execute_reply": "2025-12-13T11:25:59.890584Z",
     "shell.execute_reply.started": "2025-12-13T11:25:59.118919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# GO term loading and Ensemble prediction class\n",
    "def load_go_terms(obo_path):\n",
    "    \"\"\"Parses GO.obo to map GO IDs to their root ontology (MFO, BPO, CCO).\"\"\"\n",
    "    go_terms = {}\n",
    "    current_term = None\n",
    "    \n",
    "    if not os.path.exists(obo_path):\n",
    "        print(f\"Warning: {obo_path} not found!\")\n",
    "        return go_terms\n",
    "    \n",
    "    with open(obo_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('[Term]'):\n",
    "                current_term = {}\n",
    "            elif current_term is not None:\n",
    "                if line.startswith('id:'):\n",
    "                    current_term['id'] = line.split(': ')[1]\n",
    "                elif line.startswith('namespace:'):\n",
    "                    namespace = line.split(': ')[1]\n",
    "                    if 'biological_process' in namespace:\n",
    "                        current_term['root'] = 'BPO'\n",
    "                    elif 'molecular_function' in namespace:\n",
    "                        current_term['root'] = 'MFO'\n",
    "                    elif 'cellular_component' in namespace:\n",
    "                        current_term['root'] = 'CCO'\n",
    "                elif line == '':\n",
    "                    if current_term.get('id') and current_term.get('root'):\n",
    "                        go_terms[current_term['id']] = current_term['root']\n",
    "                    current_term = None\n",
    "    return go_terms\n",
    "\n",
    "class ProteinPredictions:\n",
    "    \"\"\"Stores and merges predictions from multiple sources.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.predictions = {}\n",
    "\n",
    "    def add_prediction(self, protein, go_term, score, branch, bonus=0):\n",
    "        if protein not in self.predictions:\n",
    "            self.predictions[protein] = {'CCO': {}, 'MFO': {}, 'BPO': {}}\n",
    "        \n",
    "        score = float(score)\n",
    "\n",
    "        if go_term in self.predictions[protein][branch]:\n",
    "            current_score = self.predictions[protein][branch][go_term]\n",
    "            new_score = max(current_score, score) + bonus\n",
    "            self.predictions[protein][branch][go_term] = new_score\n",
    "        else:\n",
    "            self.predictions[protein][branch][go_term] = score\n",
    "\n",
    "        if self.predictions[protein][branch][go_term] > 1:\n",
    "            self.predictions[protein][branch][go_term] = 1\n",
    "\n",
    "    def get_predictions(self, output_file='submission_ensemble.tsv', top=35):\n",
    "        \"\"\"Exports the merged predictions to a CAFA-formatted file.\"\"\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            for protein, branches in tqdm(self.predictions.items(), desc=\"Writing Final Submission\"):\n",
    "                for branch, go_terms in branches.items():\n",
    "                    top_go_terms = sorted(go_terms.items(), key=lambda x: x[1], reverse=True)[:top]\n",
    "                    \n",
    "                    for go_term, score in top_go_terms:\n",
    "                        f.write(f\"{protein}\\t{go_term}\\t{score:.6f}\\n\")\n",
    "\n",
    "        print(f\"\\nEnsemble CAFA submission saved to: {output_file}\")\n",
    "\n",
    "# Load GO terms\n",
    "GO_OBO_PATH = '../data/Train/go-basic.obo'\n",
    "go_terms_dict = load_go_terms(GO_OBO_PATH)\n",
    "print(f\"Loaded {len(go_terms_dict)} GO terms from ontology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T11:07:41.907994Z",
     "iopub.status.busy": "2025-12-13T11:07:41.907640Z",
     "iopub.status.idle": "2025-12-13T11:07:41.918596Z",
     "shell.execute_reply": "2025-12-13T11:07:41.917588Z",
     "shell.execute_reply.started": "2025-12-13T11:07:41.907969Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to make CAFA submission with SVM\n",
    "def make_cafa_submission_batch_svm(\n",
    "    test_ids,\n",
    "    X_test_scaled,\n",
    "    svm_mf, mf_terms,\n",
    "    svm_bp, bp_terms,\n",
    "    svm_cc, cc_terms,\n",
    "    output_path=\"submission_svm.tsv\",\n",
    "    threshold=0.0,\n",
    "    batch_size=1000\n",
    "):\n",
    "    \"\"\"Generate CAFA-format submission file from SVM predictions\"\"\"\n",
    "    N = len(test_ids)\n",
    "    \n",
    "    def get_scores(svm_model, X_batch):\n",
    "        \"\"\"Return decision function scores for each label in MultiOutputClassifier\"\"\"\n",
    "        score_list = [est.decision_function(X_batch) for est in svm_model.estimators_]\n",
    "        scores = np.column_stack(score_list)\n",
    "        return scores\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for start in tqdm(range(0, N, batch_size), desc=\"Processing proteins\"):\n",
    "            end = min(start + batch_size, N)\n",
    "\n",
    "            X_batch = X_test_scaled[start:end]\n",
    "            ids_batch = test_ids[start:end]\n",
    "\n",
    "            # Get decision scores\n",
    "            scores_mf = get_scores(svm_mf, X_batch)\n",
    "            scores_bp = get_scores(svm_bp, X_batch)\n",
    "            scores_cc = get_scores(svm_cc, X_batch)\n",
    "            \n",
    "            # Write predictions\n",
    "            for i, pid in enumerate(ids_batch):\n",
    "                ontologies = [\n",
    "                    (scores_mf[i], mf_terms),\n",
    "                    (scores_bp[i], bp_terms),\n",
    "                    (scores_cc[i], cc_terms)\n",
    "                ]\n",
    "                \n",
    "                for scores, terms in ontologies:\n",
    "                    mask = scores >= threshold\n",
    "                    for go, score in zip(np.array(terms)[mask], scores[mask]):\n",
    "                        f.write(f\"{pid}\\t{go}\\t{score:.6f}\\n\")\n",
    "\n",
    "    print(f\"\\nCAFA submission saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions and create ensemble submission\n",
    "CTD_SUBMISSION_FILE = \"../data/checkpoint/submissions/submission_svm.tsv\"\n",
    "\n",
    "# Generate SVM predictions\n",
    "print(\"Generating SVM predictions...\")\n",
    "make_cafa_submission_batch_svm(\n",
    "    test_ids,\n",
    "    X_test_scaled,\n",
    "    svm_mf, mf_terms_filtered,\n",
    "    svm_bp, bp_terms_filtered, \n",
    "    svm_cc, cc_terms_filtered, \n",
    "    output_path=CTD_SUBMISSION_FILE, \n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "# Initialize ensemble\n",
    "predictor = ProteinPredictions()\n",
    "\n",
    "# Load SVM predictions\n",
    "print(f\"\\nLoading SVM predictions from: {CTD_SUBMISSION_FILE}\")\n",
    "if os.path.exists(CTD_SUBMISSION_FILE):\n",
    "    with open(CTD_SUBMISSION_FILE, 'r') as f:\n",
    "        for item in tqdm(f, desc=\"Processing SVM scores\"):\n",
    "            try:\n",
    "                protein_id, go_term, score_str = item.strip().split('\\t')\n",
    "                score = float(score_str)\n",
    "                if go_term in go_terms_dict:\n",
    "                    root = go_terms_dict[go_term]\n",
    "                    predictor.add_prediction(protein_id, go_term, score, root, bonus=0.0)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Export final ensemble submission\n",
    "    FINAL_SUBMISSION_FILE = \"../data/checkpoint/submissions/submission_svm_final.tsv\"\n",
    "    predictor.get_predictions(FINAL_SUBMISSION_FILE, top=35)\n",
    "    print(f\"\\n✓ Submission file created: {FINAL_SUBMISSION_FILE}\")\n",
    "else:\n",
    "    print(f\"Error: {CTD_SUBMISSION_FILE} not found!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3590060,
     "sourceId": 6247561,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8688768,
     "sourceId": 13665986,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8984734,
     "sourceId": 14106315,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8984889,
     "sourceId": 14106507,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
